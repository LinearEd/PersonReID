FAST REID
https://github.com/JDAI-CV/fast-reid

DG-NET
https://github.com/NVlabs/DG-Net -> Generates uniform images from people
https://github.com/layumi/PerceptualSimilarity -> can use the generated images from DG-NET for better image comparison
https://github.com/layumi/TTUR -> Uses the generated images to check wether they align with the dataset used to train the model. Basically say wether the generated images have high enough quality for PersonReId. Higher value = Bad; Lower Value = Good; Can not be used as is. I made my own fid.py code which you can copy paste from below.

Open-ReID
https://github.com/Cysu/open-reid

2 Stream Person ReID
https://github.com/layumi/2016_person_re-ID -> pre

Person ReID GAN
https://github.com/layumi/Person-reID_GAN -> no pretrained models

__________________________________________________________________________________________________________________________


import numpy as np
import os
import gzip, pickle
import tensorflow as tf
import PIL
from PIL import Image
from scipy import linalg
import pathlib
import urllib
import warnings

class InvalidFIDException(Exception):
    pass

h = 256
w = 128
seed = 7


def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):
    """Numpy implementation of the Frechet Distance.
    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)
    and X_2 ~ N(mu_2, C_2) is
            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).
            
    Stable version by Dougal J. Sutherland.

    Params:
    -- mu1 : Numpy array containing the activations of the pool_3 layer of the
             inception net ( like returned by the function 'get_predictions')
             for generated samples.
    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted
               on an representive data set.
    -- sigma1: The covariance matrix over activations of the pool_3 layer for
               generated samples.
    -- sigma2: The covariance matrix over activations of the pool_3 layer,
               precalcualted on an representive data set.

    Returns:
    --   : The Frechet Distance.
    """

    mu1 = np.atleast_1d(mu1)
    mu2 = np.atleast_1d(mu2)

    sigma1 += np.eye(sigma1.shape[0]) * eps
    sigma2 += np.eye(sigma2.shape[0]) * eps

    assert mu1.shape == mu2.shape, "Training and test mean vectors have different lengths"
    assert sigma1.shape == sigma2.shape, "Training and test covariances have different dimensions"

    diff = mu1 - mu2

    # product might be almost singular
    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)
    if not np.isfinite(covmean).all():
        msg = "fid calculation produces singular product; adding %s to diagonal of cov estimates" % eps
        warnings.warn(msg)
        offset = np.eye(sigma1.shape[0]) * eps
        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))

    # numerical error might give slight imaginary component
    if np.iscomplexobj(covmean):
        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):
            m = np.max(np.abs(covmean.imag))
            raise ValueError("Imaginary component {}".format(m))
        covmean = covmean.real

    tr_covmean = np.trace(covmean)

    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean
#-------------------------------------------------------------------------------


def _handle_path(path, sess, low_profile=False):
    if path.endswith('.npz'):
        f = np.load(path)
        m, s = f['mu'][:], f['sigma'][:]
        f.close()
    else:
        path = pathlib.Path(path)
        # I use subdir so I change here.
        files = list(path.glob('*/*.jpg')) + list(path.glob('*/*.png'))
        if len(files) > 12936:  # random select 12936 samples for evaluation
            np.random.seed(seed)
            rand_index = np.random.permutation(len(files))[0:12936]
            files = list(files[i] for i in rand_index)

        if low_profile:
            m, s = calculate_activation_statistics_from_files(files, sess)
        else:
            x = np.array([ np.array(Image.open(str(fn)).resize((w, h), PIL.Image.BICUBIC)) for fn in files])
            m, s = calculate_activation_statistics(x, sess)
            del x #clean up memory
    return m, s


def calculate_fid_given_paths(paths, inception_path, low_profile=False):
    inception_path = check_or_download_inception(inception_path)
    tf.compat.v1.reset_default_graph()
    create_inception_graph(str(inception_path))
    with tf.compat.v1.Session() as sess:
        sess.run(tf.compat.v1.global_variables_initializer())
        m1, s1 = _handle_path(paths[0], sess, low_profile=low_profile)
        m2, s2 = _handle_path(paths[1], sess, low_profile=low_profile)
        fid_value = calculate_frechet_distance(m1, s1, m2, s2)
        return fid_value


def create_inception_graph(pth):
    """Creates a graph from saved GraphDef file."""
    with tf.io.gfile.GFile(pth, 'rb') as f:
        graph_def = tf.compat.v1.GraphDef()
        graph_def.ParseFromString(f.read())
        _ = tf.compat.v1.import_graph_def(graph_def, name='FID_Inception_Net')

def _get_inception_layer(sess):
    layername = 'FID_Inception_Net/pool_3:0'
    pool3 = sess.graph.get_tensor_by_name(layername)
    return pool3

def create_inception_graph(pth):
    with tf.io.gfile.GFile(pth, 'rb') as f:
        graph_def = tf.compat.v1.GraphDef()
        graph_def.ParseFromString(f.read())
        _ = tf.compat.v1.import_graph_def(graph_def, name='FID_Inception_Net')

def get_activations(images, sess, batch_size=50, verbose=False):
    inception_layer = _get_inception_layer(sess)
    n_batches = images.shape[0]
    pred_arr = np.empty((n_batches, 2048))
    for i in range(n_batches):
        if verbose:
            print("\rProcessing image %d/%d" % (i + 1, n_batches), end="", flush=True)
        # Process each image individually
        batch = images[i:i+1]  # Keep the batch dimension but with a single image
        pred = sess.run(inception_layer, feed_dict={'FID_Inception_Net/ExpandDims:0': batch})
        pred_arr[i] = pred.reshape(-1)
    if verbose:
        print(" done")
    return pred_arr

def calculate_activation_statistics(images, sess, batch_size=50, verbose=False):
    act = get_activations(images, sess, batch_size, verbose)
    mu = np.mean(act, axis=0)
    sigma = np.cov(act, rowvar=False)
    return mu, sigma

def load_image_batch(files):
    return np.array([np.array(Image.open(str(fn)).resize((w, h), PIL.Image.BICUBIC)) for fn in files])

def get_activations_from_files(files, sess, batch_size=50, verbose=False):
    inception_layer = _get_inception_layer(sess)
    d0 = len(files)
    if batch_size > d0:
        print("warning: batch size is bigger than the data size. setting batch size to data size")
        batch_size = d0
    n_batches = d0//batch_size
    n_used_imgs = n_batches*batch_size
    pred_arr = np.empty((n_used_imgs,2048))
    for i in range(n_batches):
        if verbose:
            print("\rPropagating batch %d/%d" % (i+1, n_batches), end="", flush=True)
        start = i*batch_size
        end = start + batch_size
        batch = load_image_batch(files[start:end])
        pred = sess.run(inception_layer, {'FID_Inception_Net/ExpandDims:0': batch})
        pred_arr[start:end] = pred.reshape(batch_size,-1)
    if verbose:
        print(" done")
    return pred_arr

def calculate_activation_statistics_from_files(files, sess, batch_size=50, verbose=False):
    act = get_activations_from_files(files, sess, batch_size, verbose)
    mu = np.mean(act, axis=0)
    sigma = np.cov(act, rowvar=False)
    return mu, sigma

def check_or_download_inception(inception_path):
    INCEPTION_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'
    if inception_path is None:
        inception_path = '/tmp'
    inception_path = pathlib.Path(inception_path)
    model_file = inception_path / 'classify_image_graph_def.pb'
    if not model_file.exists():
        print("Downloading Inception model")
        from urllib import request
        import tarfile
        fn, _ = request.urlretrieve(INCEPTION_URL)
        with tarfile.open(fn, mode='r') as f:
            f.extract('classify_image_graph_def.pb', str(model_file.parent))
    return str(model_file)


if __name__ == "__main__":
    from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
    parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)
    parser.add_argument("path", type=str, nargs=2,
                        help='Path to the generated images or to .npz statistic files')
    parser.add_argument("-i", "--inception", type=str, default=None,
                        help='Path to Inception model (will be downloaded if not provided)')
    parser.add_argument("--gpu", default="", type=str,
                        help='GPU to use (leave blank for CPU only)')
    parser.add_argument("--lowprofile", action="store_true",
                        help='Keep only one batch of images in memory at a time. This reduces memory footprint, but may decrease speed slightly.')
    args = parser.parse_args()

    # TensorFlow 2.x doesn't use CUDA_VISIBLE_DEVICES to manage GPU visibility, this is handled by tf.config instead.
    # However, for compatibility reasons, we can still use this approach to restrict GPU usage.
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu

    # Ensure TensorFlow 1.x style graph and session are used within TensorFlow 2.x environment.
    tf.compat.v1.disable_eager_execution()  # Disables eager execution to use tf.Session as in TensorFlow 1.x

    fid_value = calculate_fid_given_paths(args.path, args.inception, args.lowprofile)
    print("FID: ", fid_value)














